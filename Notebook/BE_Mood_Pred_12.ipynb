{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14e1432",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f391a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import emoji\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import Word\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from textblob import TextBlob\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "072dcaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Disha\n",
      "[nltk_data]     Chavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Disha Chavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Disha\n",
      "[nltk_data]     Chavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b19013",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3e444762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated ðŸ˜Ÿ</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wro...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy ðŸ˜ </td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ive been taking or milligrams or times recomme...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i feel as confused about life as a teenager or...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i feel romantic too ðŸ’“</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text   Emotion\n",
       "0                         i didnt feel humiliated ðŸ˜Ÿ    sadness\n",
       "1  i can go from feeling so hopeless to so damned...   sadness\n",
       "2  im grabbing a minute to post i feel greedy wro...     anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      love\n",
       "4                            i am feeling grouchy ðŸ˜       anger\n",
       "5  ive been feeling a little burdened lately wasn...   sadness\n",
       "6  ive been taking or milligrams or times recomme...  surprise\n",
       "7  i feel as confused about life as a teenager or...      fear\n",
       "8  i have been with petronas for years i feel tha...       joy\n",
       "9                             i feel romantic too ðŸ’“       love"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"emoji_dataset.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df5cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Text','Emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c4b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = np.split(data.sample(frac=1, random_state=42), [int(.8*len(data)), int(0.9*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6917f3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e939158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b1766e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd01e92",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "18cb3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "def convert_to_lower(text):\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        words[i] = words[i].lower()\n",
    "    sentence = \" \".join(words)\n",
    "    return sentence\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    return filtered_sentence\n",
    "\n",
    "def remove_non_alphanum(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text\n",
    "\n",
    "def word_check(word):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    sentence = pattern.sub(r\"\\1\\1\", word)\n",
    "    b = TextBlob(sentence)\n",
    "    return str(b.correct())\n",
    "\n",
    "def remove_extendedwords(text):\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        words[i] = word_check(words[i])\n",
    "    sentence = \" \".join(words)\n",
    "    return sentence\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tag_map = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        wn_tag = tag_map.get(tag[0].upper(), wordnet.NOUN)\n",
    "        if wn_tag == wordnet.VERB and word.endswith('ed'):\n",
    "            wn_tag = wordnet.VERB\n",
    "            word = word[:-2]\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "\n",
    "    # join the lemmatized words back into a sentence\n",
    "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "\n",
    "    return (lemmatized_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f8f5b1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10650    noticed several month ago start feel resentful...\n",
       "2041     love lot different kind sport love hang friend...\n",
       "8668      feel even kill agonize extent loudly_crying_face\n",
       "1114     feel numb way wound really start hurt slightly...\n",
       "13902    feel happy inspire little si love read write g...\n",
       "                               ...                        \n",
       "7382     pay month month feel shame every time grill ho...\n",
       "13492         feel determine go get face_with_tears_of_joy\n",
       "10394    remember feeling bit confuse really question s...\n",
       "16865                feel helpless look world fearful_face\n",
       "5047     believe happy healthy relationship likely feel...\n",
       "Name: Text, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Text'] = train['Text'].apply(lambda x: convert_emoji(x))\n",
    "train['Text'] = train['Text'].apply(lambda x: convert_to_lower(x))\n",
    "train['Text'] = train['Text'].apply(lambda x: remove_stopwords(x))\n",
    "train['Text'] = train['Text'].apply(lambda x: remove_non_alphanum(x))\n",
    "train['Text'] = train['Text'].apply(lambda x: lemmatize_text(x))\n",
    "train.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "480de5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3716     didnt want lazy feel groggy keep drinking red ...\n",
       "10837    thought good idea give time recover feel nervo...\n",
       "6140     feel like didnt really care alexis irritate ev...\n",
       "9956     feel stress free heading holiday rolling_on_th...\n",
       "1549     keep feeling sometimes one fake till make cryi...\n",
       "                               ...                        \n",
       "11284    want savor feel ecstatic anticipation abide da...\n",
       "11964    im feel puppy dog rainbows im exhaust yes beli...\n",
       "5390                                 feel delicate bouquet\n",
       "860                start feel little stressed broken_heart\n",
       "15795    feel stress tired worn shape neglect worried_face\n",
       "Name: Text, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Text'] = test['Text'].apply(lambda x: convert_emoji(x))\n",
    "test['Text'] = test['Text'].apply(lambda x: convert_to_lower(x))\n",
    "test['Text'] = test['Text'].apply(lambda x: remove_stopwords(x))\n",
    "test['Text'] = test['Text'].apply(lambda x: remove_non_alphanum(x))\n",
    "test['Text'] = test['Text'].apply(lambda x: lemmatize_text(x))\n",
    "test.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a2f8a99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1262     go detail long night wake bus feel like could ...\n",
       "19010        im feeling torture write today grimacing_face\n",
       "7212            still feel like tragic waste pleading_face\n",
       "975      feel humiliate body husband make advance towar...\n",
       "2566                   feel horribly insecure fearful_face\n",
       "                               ...                        \n",
       "10900      feel helpless make real difference pensive_face\n",
       "7758     feel impatient much thanks nic know calm face_...\n",
       "4837     feel outrage life easy bless angry_face_with_h...\n",
       "6548     feel like witness birth really amazing dm face...\n",
       "4481     flip guy feel terrible today flip guy feel ter...\n",
       "Name: Text, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate['Text'] = validate['Text'].apply(lambda x: convert_emoji(x))\n",
    "validate['Text'] = validate['Text'].apply(lambda x: convert_to_lower(x))\n",
    "validate['Text'] = validate['Text'].apply(lambda x: remove_stopwords(x))\n",
    "validate['Text'] = validate['Text'].apply(lambda x: remove_non_alphanum(x))\n",
    "validate['Text'] = validate['Text'].apply(lambda x: lemmatize_text(x))\n",
    "validate.Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51618b3",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "40539f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12126 unique words.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_texts = train['Text']\n",
    "tokenizer = Tokenizer(15212,lower=True,oov_token='UNK')\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))\n",
    "\n",
    "# texts_to_sequences: Transforms each text in texts to a sequence of integers. \n",
    "# It basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n",
    "\n",
    "train_texts_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "\n",
    "# pad_sequences: Ensure that all sequences in a list have the same length. \n",
    "train_texts_pad_sequences = pad_sequences(train_texts_sequences, maxlen=80, padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3f5687bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 0, 5, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Emotion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "454edf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "emotions = {'sadness': 0, 'joy': 1, 'surprise': 2, 'love': 3, 'anger': 4, 'fear': 5}\n",
    "\n",
    "# Step 1: Replace all emotion values with integers\n",
    "train['Emotion'] = train.Emotion.replace(emotions)\n",
    "train_emotion_integers = train['Emotion'].values\n",
    "\n",
    "# Step 2: Changing the integers to binary\n",
    "train_emotion_categorical = to_categorical(train_emotion_integers)\n",
    "train_emotion_categorical[:6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cc53badb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_texts = validate['Text']\n",
    "validate_emotion_integers = validate.Emotion.replace(emotions)\n",
    "validate_texts_sequences = tokenizer.texts_to_sequences(validate_texts)\n",
    "validate_texts_pad_sequences = pad_sequences(validate_texts_sequences, maxlen=80, padding='post')\n",
    "validate_emotion_categorical = to_categorical(validate_emotion_integers.values)\n",
    "validate_emotion_categorical[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e756b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "  tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "  tpu_strategy = tf.distribute.get_strategy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0ec3f90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 80, 64)            973568    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 80, 64)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 80, 160)          92800     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 320)              410880    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 1926      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,479,174\n",
      "Trainable params: 1,479,174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,Dense,Embedding,Dropout\n",
    "\n",
    "# instantiating the model in the strategy scope creates the model on the TPU\n",
    "with tpu_strategy.scope():\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(15212,64,input_length=80))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Bidirectional(LSTM(80,return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(160)))\n",
    "    model.add(Dense(len(emotions),activation='softmax'))\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "15b2b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d8852a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 100s 189ms/step - loss: 0.5632 - accuracy: 0.7919 - val_loss: 0.1478 - val_accuracy: 0.9560\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 99s 198ms/step - loss: 0.1229 - accuracy: 0.9609 - val_loss: 0.0782 - val_accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 101s 201ms/step - loss: 0.0975 - accuracy: 0.9702 - val_loss: 0.0678 - val_accuracy: 0.9810\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 104s 209ms/step - loss: 0.0446 - accuracy: 0.9851 - val_loss: 0.0752 - val_accuracy: 0.9765\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 106s 212ms/step - loss: 0.0302 - accuracy: 0.9898 - val_loss: 0.0477 - val_accuracy: 0.9820\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 107s 214ms/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.0612 - val_accuracy: 0.9795\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 105s 210ms/step - loss: 0.0200 - accuracy: 0.9931 - val_loss: 0.0541 - val_accuracy: 0.9825\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.0136 - accuracy: 0.9961 - val_loss: 0.0922 - val_accuracy: 0.9760\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 106s 213ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0574 - val_accuracy: 0.9815\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 103s 207ms/step - loss: 0.0142 - accuracy: 0.9960 - val_loss: 0.0558 - val_accuracy: 0.9830\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(train_texts_pad_sequences, train_emotion_categorical, epochs=10, validation_data = (validate_texts_pad_sequences, validate_emotion_categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3f65ff47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = test['Text']\n",
    "test_emotion_integers = test.Emotion.replace(emotions)\n",
    "test_texts_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_texts_pad_sequences = pad_sequences(test_texts_sequences, maxlen=80, padding='post')\n",
    "test_emotion_categorical = to_categorical(test_emotion_integers.values)\n",
    "test_emotion_categorical[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "605669f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 6s 97ms/step - loss: 0.0518 - accuracy: 0.9845\n",
      "[0.051815781742334366, 0.984499990940094]\n"
     ]
    }
   ],
   "source": [
    "x = model.evaluate(test_texts_pad_sequences, test_emotion_categorical)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3fe6f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(value):\n",
    "    for key,val in emotions.items():\n",
    "          if (val==value):\n",
    "            return key\n",
    "\n",
    "\n",
    "        \n",
    "def predict(sentence):\n",
    "    sentence = convert_emoji(sentence)\n",
    "    sentence = convert_to_lower(sentence)\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    sentence = remove_non_alphanum(sentence)\n",
    "    sentence = remove_extendedwords(sentence)\n",
    "    sentence = lemmatize_text(sentence)\n",
    "    print(sentence)\n",
    "    sentence_lst=[]\n",
    "    sentence_lst.append(sentence)\n",
    "    sentence_seq=tokenizer.texts_to_sequences(sentence_lst)\n",
    "    sentence_padded=pad_sequences(sentence_seq,maxlen=80,padding='post')\n",
    "    certaintyprediction = model.predict(sentence_padded)[0]\n",
    "    for key,val in emotions.items():\n",
    "          print(key + ': ' + str(round(certaintyprediction[val]*100, 2)) + ' %')\n",
    "    bestpredictionindex = np.argmax(certaintyprediction)\n",
    "    certainty = str(round(certaintyprediction[bestpredictionindex]*100, 2))\n",
    "    print('\\nI am '+ certainty + ' % sure the emotion is ' + get_key(bestpredictionindex) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "85b5aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rude\n",
      "sadness: 42.2 %\n",
      "joy: 2.12 %\n",
      "surprise: 1.81 %\n",
      "love: 2.93 %\n",
      "anger: 47.7 %\n",
      "fear: 3.25 %\n",
      "\n",
      "I am 47.7 % sure the emotion is anger.\n"
     ]
    }
   ],
   "source": [
    "predict(\"You are being very rude.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9251eadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "sadness: 69.7 %\n",
      "joy: 8.07 %\n",
      "surprise: 1.94 %\n",
      "love: 2.37 %\n",
      "anger: 16.44 %\n",
      "fear: 1.48 %\n",
      "\n",
      "I am 69.7 % sure the emotion is sadness.\n"
     ]
    }
   ],
   "source": [
    "predict(\"I am very happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593ca8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
